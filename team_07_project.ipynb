{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3b3d1df930f4fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data Science Project SoSe 2024\n",
    "## Team 07\n",
    "- Maximilian Hoffmann\n",
    "- Kilian Kempf\n",
    "- Daniel Schneider\n",
    "- Tom Schuck\n",
    "\n",
    "## Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897f6035bf1f06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Notebook content\n",
    "\n",
    "- Data Initialization\n",
    "- General Analysis\n",
    "- Feature Engineering\n",
    "- Feature Analysis\n",
    "- Model Training\n",
    "- Model Evaluation\n",
    "- Accuracy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55107c99d2befa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:33.388726700Z",
     "start_time": "2024-06-24T14:49:32.912640400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data_management import DataManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8a9f075a89c74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Initialization\n",
    "To ensure that all tasks like the analysis and the feature engineering are conducted on the same, consistent dataset, a custom data manager class has been created. The data manager provides methods, that are needed and can be re-used in multiple tasks (e.g. retrieval of orders with tip information for the training dataset). Since the data management functionality is encapsulated in the data manager, the room for errors caused by redundant implementations is reduced and the code is more maintainable, because methods can be adjusted in a single location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c60b428dda39c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:50:37.744536800Z",
     "start_time": "2024-06-24T14:49:34.242698800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data/Instacart')\n",
    "\n",
    "op_prior = pd.read_csv(os.path.join(DATA_DIR, 'order_products__prior.csv.zip'))\n",
    "op_train = pd.read_csv(os.path.join(DATA_DIR, 'order_products__train.csv.zip'))\n",
    "\n",
    "tip_train = pd.read_csv(os.path.join(DATA_DIR, 'tip_trainingsdaten1_.csv'))[['order_id', 'tip']]\n",
    "tip_test = pd.read_csv(os.path.join(DATA_DIR, 'tip_testdaten1_template.csv'))\n",
    "\n",
    "orders = pd.read_csv(os.path.join(DATA_DIR, 'orders.csv.zip'))\n",
    "aisles = pd.read_csv(os.path.join(DATA_DIR, 'aisles.csv.zip'))\n",
    "departments = pd.read_csv(os.path.join(DATA_DIR, 'departments.csv.zip'))\n",
    "products = pd.read_csv(os.path.join(DATA_DIR, 'products.csv.zip'))\n",
    "\n",
    "data_manager = DataManager(op_prior, op_train, tip_train, tip_test, orders, products, aisles, departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40021b5b67564c0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Analysis\n",
    "Several analysis were done to find any tip patterns in the data. The purpose of this analysis is to find specific features that are relevant for the prediction of tip. To find out which attributes have a significant influence on the tip, several analysis tasks were executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ae1ab96cfd44c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from analysis import ReorderedAnalysis, ProductCartOrder, GeneralFacts, DayOfWeek, HourOfDay, Department, Aisle, \\\n",
    "    Product, OrderNumber, GeneralAnalysis, NumberOrderUser, DaysSincePriorOrder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa8045a107981b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "TODO: Describe here your analysis task. What is the analysis about, any assumptions before? What did we expect?\n",
    "TODO: Describe the results also here in this Markdown\n",
    "\n",
    "\n",
    "\n",
    "##### **Analysis Task Name**\n",
    "**Analysis:** \n",
    "\n",
    "**Result Interpretation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1a6cc550d926c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize your analysis instance\n",
    "# TODO: Produce your output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20583388f6fdb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **General Analysis**\n",
    "**Analysis:** The general analysis task was created to get an overview of the data set and to find out general facts about the dataset and its data. Typical functions like the mean, median, standard deviation, min and max values were calculated for the different entities. The purpose of this analysis task was to get a first impression of the data set and to find out if there are any irregularities or outliers in the data. The datatypes of the different attributes were checked, and the number of missing values was calculated.\n",
    "\n",
    "**Result Interpretations:** \n",
    "- The dataset mostly contains numerical attributes with three categorical attributes, namely product_name, aisle, and department.\n",
    "- The dataset is rather large with 32 million rows and 16 columns.\n",
    "- The dataset contains some missing values in the columns \"days_since_prior_order\" and some in \"tip\". The missing \"tip\" values are limited to the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c9bf680f46960",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "general_analysis = GeneralAnalysis(data_manager)\n",
    "general_analysis.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ac911e6d2998a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **General Facts**\n",
    "**Analysis:** For data understanding and getting familiar with the data set and the overall relations between the main entities, some general facts about the dataset were collected. Therefore several facts has been calculated containing the different entities and there relations.\n",
    "\n",
    "**Result Interpretations:** The results are important to clarify weather the calculations fit with the overall facts of the dataset and do not contain any calculation errors or changed the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1dae5125333d5e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "general_facts = GeneralFacts(data_manager)\n",
    "general_facts.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f278cd782979103",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Day of the Week Analysis**\n",
    "**Analysis:** The analysis was conducted to find out whether the day of the week has an impact on the tip probability. To find out if there is a connection between the day of the week and the tipping behaviour, 2 plots were created.\n",
    "- Left plot: The left plot shows the absolute number of orders that were tipped and not tipped depending on the day of the week on which the order was placed as a bar chart with overlapping bars. This was done to get an overview of how the orders are distributed over the days of the week, to put the tip probability into perspective.\n",
    "- Right plot: The plot on the right shows the probability that an order was tipped depending on the day of the week on which the order was placed. The tip probability was calculated by dividing the number of tipped orders by the total number of orders for each day of the week.\n",
    "\n",
    "**Result Interpretation:**\n",
    "The right plot shows that the tip probability is significantly higher on day 0 and 1 (probably Saturday and Sunday) than on the other days of the week. While the tip probability is relatively constant on the other days of the week at or below 40%, the tip probability on day 0 and 1 is above 50%.\n",
    "\n",
    "&rarr; **Impact of order_dow**: The feature **dow_high_tip_probability**, which one-hot encodes whether the order was placed on the days on saturday/sunday or not, was created to capture this relationship. This was necessary, because the order_dow is an ordinal-scaled variable, which cannot be used directly in most models (e.g. logistic regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b3ca8e9a5c61d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "day_of_week = DayOfWeek(data_manager)\n",
    "day_of_week.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303fd5dc7e7ccc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Hour of the Day Analysis**\n",
    "**Analysis:** The analysis was conducted to find out whether the hour of the day, in which the order has been placed, has an impact on the probability that the order is tipped. To find out if there is a connection, the same plots were created as in the day of the week analysis, only this time with the hour of the day.\n",
    "\n",
    "**Result Interpretation:**\n",
    "The right plot shows that the tip probability is significantly higher in the early morning and in the evening than during the day. The period in the early morning, where the tip probability is higher, includes the hours 0-4 (from 12 until 5 a.m.). At hour 5 a significant drop in the tip probability can be observed. The tip probability remains relatively constant during the day with a slight increase in the morning hours. Since the change in the tip probability is not as drastic as in the early morning and doesn't reach the same level as in the early morning or the evening, this increase is not considered significant. At hour 19, the tip probability increases abruptly and remains at a high level until hour 23 (7 p.m. until 12 a.m.). Therefore, the hours 0-4 and 19-23 are considered as the hours with a high tip probability.\n",
    "\n",
    "In the left plot, it can be observed that the number of placed orders is distributed very unevenly over the hours of the day. In the hours with a high tip probability, the number of orders is significantly lower than during the day. This is particularly noticeable in the early morning hours, where the number of orders is very low. Therefore, the significant increase in the tip probability in the early morning hours is not as relevant as the increase in the evening, where the number of orders is higher. Despite this fact, no differentiation was made between the early morning and the evening, for simplicity reasons and since the tip probability is very similar in both periods, which cover a continuous time span (7 p.m. until 5 a.m.) when combined.\n",
    "\n",
    "&rarr; **Impact of order_dow**: The feature **hod_high_tip_probability**, which one-hot encodes whether the order was placed in the hours with a high tip probability (0-4 and 19-23) or not, was created to capture this relationship. This is necessary because the order_hour_of_day is also an ordinal-scaled variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56abb6c081f537",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hour_of_day = HourOfDay(data_manager)\n",
    "hour_of_day.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e962ca5a25c54e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Order Number Analysis**\n",
    "**Analysis:** The analysis of the order number was conducted to find out whether the attribute has an impact on the tip probability. Again, the same plots were created as in the day of the week analysis, only this time with the order number. To better visualize the relationship, a line graph was chosen to show the tip probability in dependence of the order number, instead of a bar plot.\n",
    "\n",
    "**Result Interpretation:**\n",
    "The right plot shows that the probability that an order is tipped increases with a rising order number up until around the 40th order. After that, the tip probability decreases again. In the graph one can clearly see that the curve is not linear, but rather resembles the shape of a quadratic function.\n",
    "\n",
    "The left plot shows a decreasing number of orders with a rising order number. This was expected due to the fact, that the order number is a user-specific attribute and there are more users with a shorter order history than with a longer order history. Therefore, an order with a larger order number is less likely to appear. This also reduces the significance of the tip probability in the right plot for higher order numbers, because the number of orders is very low. However, this is not encoded explicitly in the feature, but will be left to the model to learn an appropriate weight.\n",
    "\n",
    "&rarr; **Impact of order_number**: The feature **order_number_squared** was created, which contains the squared order number. This feature was created to allow linear models to capture the quadratic relationship between the order number and the tip probability, when combined with the original order number feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0fd1f12995cb6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "order_number = OrderNumber(data_manager)\n",
    "order_number.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc47320261a3fc1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Days Since Prior Order Analysis**\n",
    "\n",
    "**Analysis:** The analysis was conducted to find out whether the days since the prior order has an impact on the tip probability. To find out if there is a connection, the same plots were created as in the analysis of the order number, only this time with the days since the prior order.\n",
    "\n",
    "**Result Interpretation:**\n",
    "The right plot shows that the tip probability is growing the longer the user waits between two orders until the peak is reached at 7 days since the user placed an order. After the 7th day, the tip probability decreases more or less constantly with some peaks at the 14th, 21st and 28th day since the user placed his/her last order. The lowest tip probability is reached at the 30th day since the user placed his/her last order. That the highest tip probability is reached at the 7th day since the user placed his/her last order could be explained that user order items, which they especially like on a weekly basis and therefore tip more often. This also explains the little peaks at the 14th, 21st and 28th day since the user placed his/her last order, when users order some items regularly every 2, 3 or 4 weeks.\n",
    "\n",
    "The right plot supports this assumption as there are more orders that are placed with the aforementioned values for the days since the prior order compared to the other values. A local maximum can be observed at 7 days since prior order with some smaller peaks at 14, 21 and 28 days since prior order.\n",
    "The global maximum can be observed at 30 days since prior order, which is the most frequent value for the days since prior order. One possible explanation for this unusual peak could be that there is a kind of subscription model, where items are ordered automatically every 30 days. This would explain the high number of orders at 30 days since prior order and the low tip probability, because the user did not actively place the order.\n",
    "The more likely explanation is that 30 is used as value for the days since prior order, when the user has not placed an order for a long time and the actual days since prior order is unknown or larger than 30. This is also supported by the general analysis, which shows, that 30 is the maximum value for the days since prior order. The low tip probability  could then be explained, that users, who order very rarely are less likely to tip.\n",
    "\n",
    "&rarr; **Impact of days_since_prior_order**: The feature **days_since_prior_order** was kept in the feature set, since it has a significant impact on the tip probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3bb5f602369b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "days_since_prior_order = DaysSincePriorOrder(data_manager)\n",
    "days_since_prior_order.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c291367b5c6503a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Department Analysis**\n",
    "**Analysis:** The analysis of the department was conducted to find out whether orders containing products from a specific department have a higher tip probability than others. The same 2 plots were used to visualize, on one hand, the absolute number of tipped and not tipped orders containing products from a specific department and on the other hand the tip probability of orders containing products from a specific department.\n",
    "\n",
    "**Result Interpretation:** The right plot shows that the tip probability of orders containing products from the alcohol department is significantly higher than the tip probability of orders containing products from other departments. Even though the tip probability of orders containing products from the other departments is not constant, the differences between the departments are not as significant as the difference between the alcohol department and the other departments.\n",
    "\n",
    "The left plot shows that the number of orders containing products from the alcohol department is rather low compared to the other departments. But since the difference in the tip probability is so striking, it was decided to create a feature which encodes this relationship anyway.\n",
    "\n",
    "&rarr; **Impact of department**: The feature **contains_alcohol** was created, which one-hot encodes whether the order contains at least 1 product from the alcohol department or not. This feature was created to capture the significant difference in the tip probability of orders containing products from the alcohol department compared to orders containing products from other departments, without one-hot-encoding all departments. This would have resulted in 21 additional features (1 for every department), which would have lead to a very sparse feature matrix and would have made the model more complex. Additionally, the feature **dept_tip_rate**, which is described in the feature analysis section, was created to capture the relationship between the tip probability and the departments in a more generic and dense way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcec143c0d361f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "departments = Department(data_manager)\n",
    "departments.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118406b4c423ab25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Aisle Analysis**\n",
    "**Analysis:** The analysis of the aisle was conducted to find out whether orders containing products from a specific aisle have a higher tip probability than others. For this analysis, a single plot was created, which shows the tip probability of orders containing products from a specific aisle. In addition to that, aisles belonging to the alcohol department were highlighted.\n",
    "\n",
    "**Result Interpretation:** The plot reveals that orders including items from the alcohol aisles have a notably higher likelihood of receiving tips compared to orders that contain items from other aisles. While the tip probability for non-alcohol aisles varies, none exhibit a distinctly higher likelihood of tipping, unlike the alcohol aisles. Generally, there is a mild fluctuation in tip probability across these other aisles, with some slightly higher and others lower, but these variations are not as pronounced as those observed for the alcohol aisles.\n",
    "\n",
    "&rarr; **Impact of aisle**: The analysis did not reveal any significant relationship between a specific aisle, other than the alcohol aisles, which are already encoded in the **contains_alcohol** feature. Therefore, no feature was created to capture the relationship between the tip probability and a specific aisle. One-hot-encoding all 134 aisles would also not be an option. Instead, **aisles_tip_rate** was created, which is described further in the feature analysis section and encodes the fluctuation of the tip probability across all aisles in a more generic and dense way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa242e911ea114",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "aisles = Aisle(data_manager)\n",
    "aisles.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a1ed27125d379",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Product Analysis**\n",
    "**Analysis:** The analysis of the product was conducted to find out whether specific products have a higher tip probability than others. For this analysis, multiple plots and tables were created to visualize the tip probability of the products:\n",
    "\n",
    "- First plot: The first plot visualizes the distribution of products depending on the probability, that orders containing these products are tipped. The products are grouped into 100 bins, which means that every bin contains products with a tip probability between 0 and 0.01, 0.01 and 0.02, and so on. The plot shows the number of products in each bin.\n",
    "- Second plot: The second plot visualizes the same distribution, but this time the products were weighted by how often they were ordered. This was done to judge the significance of the tip probability of the products. The plot shows the number of orders containing products in each bin.\n",
    "\n",
    "- Third plot: The third plot displays the same distribution of products as the first plot, but in a more fine grained way. Only products with a tip probability between 80% and 100% are shown, grouped into 20 bins.\n",
    "- Fourth plot: The fourth plot visualizes the same distribution as the third plot, but the products were weighted by how often they were ordered.\n",
    "- Table 1: The table shows products with a 100% tip probability, that don't belong to the alcohol department, sorted by the number of orders containing these products.\n",
    "\n",
    "- Fifth plot: The fifth plot is equivalent to the third plot, but only products with a tip probability between 0% and 20% are shown, grouped into 20 bins.\n",
    "- Sixth plot: The sixth plot is equivalent to the fifth plot, but the products were weighted by how often they were ordered.\n",
    "- Table 2: The table shows products with a 0% tip probability, sorted by the number of orders containing these products.\n",
    "- Last plot: The last plot shows the distribution of products with a tip probability of 0% depending on the number of orders containing these products.\n",
    "\n",
    "In all plots, products from the alcohol department are highlighted.\n",
    "\n",
    "**Result Interpretation:**\n",
    "When looking at the first and the second plot, one can see, that the tip probability varies between different products. The distribution of the tip probability of the products is not uniform, but rather resembles a normal distribution, except there are noticeable peaks at 0% and 100%. Once the products are weighted by how often they were ordered, the peaks at 0% and 100% disappear and the distribution becomes more uniform. This leads to the conclusion that these products are not ordered very often and therefore the tip probability is not very significant. Only a small elevation can be observed starting at around 80% tip probability, which indicates that there are some products with a high tip probability, that are ordered more often than others.\n",
    "\n",
    "In the third and forth plot, this is examined in more detail. The unweighted plot on the left shows that there are many products, which do not belong to the alcohol department and have a tip probability between 80% and 100%. There is also a large peak of products, that are tipped every time they are ordered. However, when looking at the weighted plot on the right, this peak shrinks drastically. It also becomes clear that the alcohol products are extremely dominant in this area and that the other products are not ordered very often, when looking at the weighted plot. This leads to the conclusion that the tip probability of the alcohol products is rather significant, while the tip probability of the other products is not. The table 1 confirms this, as it shows that the products, which do not belong to the alcohol department and have a 100% tip probability, appear in 7 orders at most. When considering that in the dataset as a whole, products are ordered an average of 653 times, with a median order frequency of 60 times, it becomes clear that the non-alcohol products with a 100% tip probability are not very significant. Therefore, a feature using a curated list of products that are tipped every time (or above 80%) would not be very useful, since most of these products are alcohol products or are not ordered very often.\n",
    "\n",
    "A similar observation can be made for the products with a tip probability between 0% and 20%. The unweighted plot on the left shows that there are many products with a low tip probability with a significant peak at 0%. But when looking at the weighted plot on the right, it becomes clear that these products are also not ordered very often, as the peak at 0% shrinks drastically. Table 2 shows, that the products with a 0% tip probability are ordered 22 times at most, which is more significant than the products with a 100% tip probability, but still low. The last plot gives a more detailed view of the distribution of the products with a 0% tip probability. It shows that the products with a 0% tip probability are ordered very rarely, with the majority of them being ordered only 3 times. Not many of these products are ordered more than 10 times. Therefore, a feature using a curated list of products that are never tipped would also not be very useful.\n",
    "\n",
    "&rarr; **Impact of product**: The analysis showed that products that are always/never tipped are not very significant, since they are not ordered very often. Therefore, no feature was created to capture the relationship between the tip probability and a curated list of products that are always/never tipped. To allow the model to utilize the information that some products are more likely to be tipped than others, which is clearly visible in the first/second plot, the generic feature **product_tip_rate** was created, which is described further in the feature analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620012faeb36ead6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "products = Product(data_manager)\n",
    "products.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6bf40ba43f416",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Add to cart order analysis per Product**\n",
    "**Analysis:** To find out whether the add to cart order of a specific product has an impact on the tip probability of that product, an analysis task has been created which calculates the tip probability of the products in dependence of the add_to_cart_order. To compare the \"add to cart order\" of a product, the rational scaled variable (add_to_cart_order) has to be normalized. With that normalization every \"add to cart order\" is transformed to an additional normalized_add_to_cart attribute order within the interval [0,1]. After the calculation of the normalized attribute, the variable has been subdivided in 10 bins. Within these bins the mean tip probability of every product is calculated and visualized within the box plot below.\n",
    "\n",
    "**Result Interpretations:** The boxplot below shows the distribution of the product specific tip probabilities in dependence of the normalized \"add to cart order\". It displays the median, potential outliers and the area between the 25th and the 75th quartile which represents 50% of the data. Considering the area between the 25th and 75th quartile, it can be recognized that there is no overall impact of the normalized add_to_cart_order on the product specific tip probability. Focussing the outliers, it can be seen that there are some differences between the different bins. However, because the range between the 25th and 75th percentile is so constant, it can be assumed that these outliers are not necessarily related to \"add to card or\" but are due to other influences.\\\n",
    "&rarr; **No overall impact of add_to_cart_order on the tip probability of the products** &rarr; no feature crafted and no further analysis on the attribute \"add_to_cart_order\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8df4bb3defbc47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "product_cart_order = ProductCartOrder(data_manager)\n",
    "product_cart_order.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecefd8428ba8f450",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Reordered tip probability analysis per Product**\n",
    "**Analysis:** To find out weather the amount of reorders has an influence on the tip probability of a product, an analysis has been done which compares the product specific tip probability in dependence of the product reordered probability. Because of the fact that the \"reordered\" attribute is a user specific flag, the calculated product reordered rate describes the probability that a product is reordered. With a high reordered rate we can assume that the product has been bought quite often across all users. The heatmap below shows the product tip probability in comparison to the reordered rate. The purpose of this analysis task was to identify weather the reordered rate has an overall influence of the tip probability. The assumption here was that the tip probability of the products generally increases with a rising reordered rate.\n",
    "\n",
    "**Result Interpretations:** The heatmap overall shows that there is no general connection between the reordered rate and the product specific tip probability. As the reordered rate increases, the product tip probability of the products remains relatively constant and does not change depending on the reordered rate.\\\n",
    "&rarr; **No overall impact of reordered rate on the tip probability of the products** &rarr; no feature crafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a7de8a6127e81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reordered_tip_rate = ReorderedAnalysis(data_manager)\n",
    "reordered_tip_rate.execute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d92429fb09b035",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Number of Orders per User**\n",
    "**Analysis:** To find out weather the number of orders per user has an influence on the tip probability, an analysis has been done which compares the tip probability in dependence on the number of orders per user. The purpose of this analysis task was to identify weather the number of orders per user has an overall influence on the tip probability. \n",
    "For the analysis, the number of orders per user has been calculated and the tip probability has been calculated for every user. The tip probability has been calculated by dividing the number of tipped orders by the number of orders of the user. The tip probability has been calculated for every user and the mean tip probability has been calculated for every number of orders per user. The results are visualized in the line plot below.\n",
    "\n",
    "**Result Interpretations:** The line plot below shows that the tip probability of the users increases constantly with the number of orders per user until it reaches a ceiling at around 50 orders per user. Afther that the tip propability starts a slight downward trend which is rather unstable. To note is that the most data points are located in the range of 0 to 50 orders per user.\n",
    "This analysis shows that the number of orders per user has a significant impact on the tip probability and should be considered as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b93667a946921",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_order_user = NumberOrderUser(data_manager)\n",
    "num_order_user.execute_analysis()\n",
    "# TODO: What feature has been crafted\n",
    "# TODO: Use format for final result\n",
    "# TODO: Explain that it's about the TOTAL number of orders per user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea741179cb63abe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature Engineering\n",
    "With the results of the general analysis task and our specific assumptions of any relation between tip and various data combinations, a few features were build that we think will make a potential impact. The explanation of each feature and corresponding impact analysis will be covered in the next chapter (Feature analysis). To execute the impact analysis of the features, it is necessary to compute them all before.\n",
    "\n",
    "For the computation of the features, it is distinguished between two types of features:\n",
    "- **Static features:** Features that are calculated user specific and consider just the orders the respective user has placed so far (future orders are never considered). These features can be computed once and do not need to be recalculated for a newly created fold in a time series cross validation.\n",
    "- **Dynamic features:** Features that aggregate cross-user information like the product tip rate of all orders. For these features we use to data of the whole dataset or fold in cross validation. Therefore, the dynamic features need to be recalculated for every new fold in the cross validation, so that the model does not get any information about the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e100183344c38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from feature_engineering.static_features import ContainsAlcohol, DowHighTipProbability, TipHistory, OrderSize, \\\n",
    "    OrderNumberSquared, PrevOrderTipped, MeanOrderedRate, CustomerLifetime, ReorderedRatio, OrderFrequency, \\\n",
    "    HodHighTipProbability, PrevTippedProductsRatio, RelDaysSinceTip, DaysSinceTip, LastTipSequence, AvgSizePrevOrders, \\\n",
    "    SimOrdersTipRatio\n",
    "from feature_engineering.dynamic_features import ProductTipRate, DepartmentTipRate, AisleTipRate, AssocRulesAisles, \\\n",
    "    AssocRulesDepartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd33dee81df140a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Static Features\n",
    "tip_history = TipHistory()\n",
    "reordered_rate = ReorderedRatio()\n",
    "order_size = OrderSize()\n",
    "prev_tipped_products_ratio = PrevTippedProductsRatio()\n",
    "customer_lifetime = CustomerLifetime()\n",
    "prev_order_tipped = PrevOrderTipped()\n",
    "order_frequency = OrderFrequency()\n",
    "mean_ordered_rate = MeanOrderedRate()\n",
    "rel_days_since_tip = RelDaysSinceTip()\n",
    "days_since_tip = DaysSinceTip()\n",
    "sim_orders_tip_ratio = SimOrdersTipRatio()\n",
    "product_tip_rate = ProductTipRate()\n",
    "order_number_squared = OrderNumberSquared()\n",
    "hod_high_tip_probability = HodHighTipProbability()\n",
    "dow_high_tip_probability = DowHighTipProbability()\n",
    "contains_alcohol = ContainsAlcohol()\n",
    "avg_size_prev_orders = AvgSizePrevOrders()\n",
    "\n",
    "# Dynamic Features\n",
    "department_tip_rate = DepartmentTipRate()\n",
    "aisle_tip_rate = AisleTipRate()\n",
    "last_tip_sequence = LastTipSequence()\n",
    "assoc_rules_departments = AssocRulesDepartments()\n",
    "assoc_rules_aisles = AssocRulesAisles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329fc4f3df8bc6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Static Features\n",
    "data_manager.register_feature(tip_history)\n",
    "data_manager.register_feature(reordered_rate)\n",
    "data_manager.register_feature(order_size)\n",
    "data_manager.register_feature(customer_lifetime)\n",
    "data_manager.register_feature(prev_order_tipped)\n",
    "data_manager.register_feature(prev_tipped_products_ratio)\n",
    "data_manager.register_feature(order_frequency)\n",
    "data_manager.register_feature(sim_orders_tip_ratio)\n",
    "data_manager.register_feature(mean_ordered_rate)\n",
    "data_manager.register_feature(last_tip_sequence)\n",
    "data_manager.register_feature(rel_days_since_tip)\n",
    "data_manager.register_feature(days_since_tip)\n",
    "data_manager.register_feature(order_number_squared)\n",
    "data_manager.register_feature(hod_high_tip_probability)\n",
    "data_manager.register_feature(dow_high_tip_probability)\n",
    "data_manager.register_feature(contains_alcohol)\n",
    "data_manager.register_feature(avg_size_prev_orders)\n",
    "\n",
    "# Dynamic Features\n",
    "data_manager.register_feature(product_tip_rate)\n",
    "data_manager.register_feature(department_tip_rate)\n",
    "data_manager.register_feature(aisle_tip_rate)\n",
    "data_manager.register_feature(assoc_rules_departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3dc4838a3772e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Because of the computing time of each feature, the calculation has been done once before and is imported to the local file system. If it is required to compute the features once again, the next two lines of code need to be executed. This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad4118961184f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# In case a recomputing of the features is necessary. Remove the comments for the next two lines of code.\n",
    "# data_manager.compute_features()\n",
    "# data_manager.export_features('data/prepared_data/computed_features.csv.zip', only_static=False)\n",
    "\n",
    "data_manager.import_features('data/prepared_data/computed_features.csv.zip', only_static=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20852a354e30e8f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature analysis TODO: General Description of plot types?\n",
    "To decide weather a feature will be used in the feature set of the machine learning model, it is necessary to analyze which features have a significant impact on tip. All features that has been computed are described and analyzed in the following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ae2c4ea7dd51e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dataset for feature analysis\n",
    "orders_tip_features = data_manager.get_orders_tip_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6366ff9f5bd05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Feature_Name**\n",
    "**Description:** Describe how the feature is calculated and what does it mean.\n",
    "\n",
    "**Analysis:** Which impact does the feature have on tip. Does it have any correlations to other features\n",
    "&rarr; **Is the feature part of the model feature set? and why (one sentence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf845a9862cc975",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Execute the feature_name.analyze_feature() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6e59af6150c10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Average size of the previous orders (avg_size_prev_orders)**\n",
    "**Description:** The feature represents the average size of the previous orders of the user.\n",
    "For this, the size of the previous orders of the user is calculated and the mean of the sizes is taken. After this the average size of the previous orders is compared to the size of the current order. The feature is calculated by subtracting the average form the current order and dividing it by the average. This leads to a number which represents the deviation of the current order size from the average size of the previous orders.\n",
    "\n",
    "**Analysis:** The feature has a correlation which is near zero. Furthermore, after the analysis of the feature, it can be seen that the feature has no significant impact on the tip rate.\n",
    "The consequence of the low correlation and analysis is that the feature is not part of the model feature set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4613a157884f6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "avg_size_prev_orders.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7ce1453858e7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Last Tip Sequence**\n",
    "**Description:** The feature represents the sum of the length of the last tip sequence. In case the last order has not been tipped, the feature value is always zero. In case the last order has been tipped, all directly consecutive tipped orders are added to the sequence. A \"last_tip_sequence\" of five means that the last five orders had all been tipped by the user.\n",
    "\n",
    "**Analysis:** The feature has a high correlation of 0.33 seem to be very significant for the prediction of tip. The plots demonstrate that there seems to be a connection between the last tipped orders and the tip probability of the current. With a particular look at the last linegraph it can be seen that there is an identifiable relationship between the \"last tip sequence\" and the \"tip probability\" of a specific order.\n",
    "\n",
    "&rarr;**The feature will be included in the model's feature set due to its significant impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d03b286e0d348d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "last_tip_sequence.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b84199dd6236bf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Tip History (tip_history)**\n",
    "**Description:** The feature encodes the tipping behaviour of the respective user in previous orders, because the assumption is that the tipping behaviour of the past will be similar in the future. The feature is calculated by dividing the number of previous tipped orders by the number of orders the user has placed so far. The resulting value is between 0 and 1. For example, a value of 0.5 means that the user tipped in 50% of his previous orders.\n",
    "\n",
    "**Analysis:** The feature has a very high positive correlation of around 0.5591 with the tip variable and is therefore very significant. The line graph depicts very well that the tip probability consistently increases linearly with the tip history, which indicates that previous tipping behaviour is a good predictor for future tipping behaviour.\n",
    "\n",
    "&rarr;**The feature will be included in the model's feature set due to its significant impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27941b51b9f5ae71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tip_history.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe006e07158d931f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Previous Order Tipped (prev_order_tipped)**\n",
    "**Description:** The feature encodes whether the previous order of the respective user has been tipped or not. The feature is binary and is 1 if the previous order has been tipped and 0 if not. The assumption was that users who tipped in the previous order are more likely to tip in the current order as well. This feature was created as a more reactive addition to the **tip_history**. For example, when users start to consistently tip after they have placed a few orders, even though they haven't tipped in the beginning (which would not be captured by the **tip_history**). This feature also captures the opposite case, where users tipped in the beginning but stopped tipping after a few orders. This feature is more reactive to changes in the tipping behaviour of the user.\n",
    " \n",
    "**Analysis:** The feature has a high positive correlation of around 0.4904 with the tip variable and is therefore very significant. The bar plot shows that the tip probability is significantly higher at around 70% when the previous order was tipped compared to when the previous order was not tipped, where the tip probability is around 30%, which confirms the assumption that users who tipped in the previous order are more likely to tip in the current order as well. It can be assumed that the feature also captures redundant information, that is already encoded in the **tip_history** feature. Nonetheless, the feature could still be useful as a more reactive addition to the **tip_history** feature.\n",
    "\n",
    "&rarr;**The feature will be included in the model's feature set due to its significant impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedaf76097276b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prev_order_tipped.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9130273de681b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Order Size (order_size)**\n",
    "**Description:** The feature represents the size of the order. The size of the order is calculated by counting the number of products in the order. For example, an order with 5 products has an order size of 5.\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.0056 with the tip variable, which is very low and therefore not significant. The little impact of the feature can be seen in the violin plot on the left, where the distribution of the order size value is almost identical for tipped and not tipped orders. The line graph on the right shows that the tip probability increases a little bit with the order size until around 10 products and then starts to decrease again. At around 60 products, the tip probability increases again with drastic fluctuations. These fluctuations can be explained by the fact that the number of orders with a size of 60 products and more is very low and makes the tip probability very volatile. This can be seen in the plot in the middle, which shows the density of the orders depending on the order size value, which is nearly zero for orders with a size of 60 products and more.\n",
    "\n",
    "One could argue that the plot on the right shows a slight quadratic relationship, because of its parabolic form between 0 and 60 orders, which is not captured by the correlation as it measures the linear relationship between 2 variables. But as the increase and decrease are very slight, the computation of an additional quadratic feature would not be justified.\n",
    "\n",
    "&rarr; **The feature will not be included in the model's feature set due to its low impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e67310048f0f63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "order_size.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144d9434e019754",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Similar tipped and none tipped orders (sim_orders_tip_ratio)**\n",
    "**Description:** The feature is an indicator of whether similar orders from a user are more likely tipped orders or none tipped orders. The order similarity is calculated based on the product similarity between the current order and every separate previous order. Therefore, every previous order is compared with the current order and has a similarity score in the range of [0,1]. If the previous order has been tipped, the similarity score is positively added to the feature value, but if the previous order has not been tipped, the score is then subtracted from the feature value. With the corresponding weighting via the order_number, the feature receives a value between -1 and 1. A value close to one would mean that similar orders were also tipped whereas a value close to -1 would mean that similar orders has not been tipped.\n",
    "\n",
    "**Analysis:** The feature has a comparatively high correlation of 0.4118 with the tip variable and therefore appears to have a significant influence. This insight can also be recognized by the plots below. With a particular look at the plot on the right, it can be seen that the tip probability is effected by the feature value. The greatest influence of the feature on the tip probability is in the range of 0.25 to 0.25, the range in which the most data points are available.\n",
    "\n",
    "&rarr; **It seems that the feature has a not negligible impact on the tip variable. The feature will therefore be included in the model's feature set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5b0368408900e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sim_orders_tip_ratio.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401c9156a52eeca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Customer Lifetime (customer_lifetime)**\n",
    "**Description:** The feature represents how long the respective user has been a customer in days. The customer lifetime is calculated by summing up the days since the first order of the user until the current order. For example, a customer who has placed his first order 100 day before the current order, the customer lifetime is 100 days. This feature was created under the assumption that the longer a user is a customer, the more likely he is to tip, because it can be assumed that the user is satisfied with the service if he/she has been a customer for a long time.\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.0779 with the tip variable, which is not very strong. The line graph on the right shows that the tip probability slightly increases with a higher customer lifetime, with a few fluctuations. The plot in the middle shows that the increase in the tip probability is encompassed by a decrease in the order density. This reduces the significance of the high tip probabilities a little, but as the density is nowhere near zero, the relationship is still valid. Because the feature has a clear linear relationship, even though it is not very strong, it was decided to include it in the model's feature set, even though the impact probably won't be very large.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its slight impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947b52ee97584c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "customer_lifetime.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb18a25b6e4221",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Reordered Ratio (reordered_ratio)**\n",
    "**Description:** The feature represents the ratio of products in the order are reorders. The reordered ratio is calculated by dividing the number of reordered products in the order by the total number of products in the order. For example, a reordered ratio of 0.5 means that 50% of the products in the order are reorders. The feature was crafted under the assumption, that a user is more likely to tip if the user orders products he/she already knows and likes again.\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.1161, which is moderate/low. The plot on the right side shows that the tip probability more steadily increases with the reordered ratio from around 30% to above 50% (except in the beginning and in the end). Therefore, a larger portion of the products, which are reorders in the order, increases the likelihood of tipping. Due to its moderate impact on tip probability, the feature will be included in the model's feature set.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its moderate impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c17fa10b9fc33",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reordered_rate.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc331c0e77ac2904",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Previously Tipped Products Ratio (prev_tipped_products_ratio)**\n",
    "**Description:** The feature represents the ratio of products in the current order that have been part of a previous order of the respective user for which the user has tipped. The previously tipped products ratio is calculated by dividing the number of products for which the user has tipped before by the total number of products in the current order. For example, a previously tipped products ratio of 0.5 means that for 50% of the products in the current order the user has tipped before at least once in his previous orders. This feature was created under the assumption that some users tip for the same products every time they order them. In contrast to the **product_tip_rate** this feature is user-specific and is supposed to capture the possible preference of the user for tipping only for specific products\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.3967 with the tip variable, which is very strong. The violin plot on the left shows that the distribution of the previously tipped products ratio value is very different for tipped and not tipped orders. Whereas, the distribution is very dense between 0 and 0.15 and for orders that have been not been tipped, the distribution is denser at higher values of the feature for orders that have been tipped. The line graph on the right also shows that the tip probability increases very steeply with the previously tipped products ratio until around 0.1 and then increases very evenly with a smaller incline.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its significant impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ab48cfdc00b46",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prev_tipped_products_ratio.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52815570579cfc16",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Contains Alcohol (contains_alcohol)**\n",
    "**Description:** The feature encodes whether the order contains a product from the alcohol department or not. The feature is binary and is 1 if the order contains at least one product from the alcohol department and 0 if not. This feature was created because the tip probability of orders containing products from the alcohol department is significantly higher than the tip probability of orders containing products from other departments (as already shown in the analysis of the departments).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.1212 with the tip variable, which is moderate/low. The bar plot on the right shows that the tip probability is significantly higher when the order contains a product from the alcohol department (around 80%) compared to when it doesn't (around 42%). The low correlation can be explained by the fact that the values of the feature are not evenly distributed, as most orders do not contain products from the alcohol department. But as the bar chart shows, the feature has a significant impact on the tip probability.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its significant impact on tip probability.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1e569fe1b0ee7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "contains_alcohol.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a43f3559f4bdd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Day of the Week with High Tip Probability (dow_high_tip_probability)**\n",
    "**Description:** The feature encodes whether the order was placed on the days 0/1 (saturday/sunday) or not. The feature is binary and is 1 if the order was placed on day 0 or 1 and 0 if not. This feature was created because the tip probability is significantly higher on these days (as already shown in the analysis of the day of the week).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.1405 with the tip variable, which is moderate/low. Like for the **contains_alcohol** feature, the correlation is not meaningfull, because the values of the feature are not evenly distributed, as most orders are not placed on day 0 or 1. The bar plot on the right shows, that the tip probability increases from around 40% to slightly above 50% when the order was placed on day 0 or 1, which is a moderate increase.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its moderate impact on tip probability.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7918d54ae395f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dow_high_tip_probability.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42119bb7a2559f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Hour of the Day with High Tip Probability (hod_high_tip_probability)**\n",
    "**Description:** The feature encodes whether the order was placed in the hours 0-4 or 19-23 (7 p.m. until 12 a.m.) or not. The feature is binary and is 1 if the order was placed in this period and 0 if not. This feature was created because the tip probability is significantly higher in this period (as already shown in the analysis of the hour of the day).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.0883, which is rather low. Again, the correlation is not meaningful and the bar plot on the right shows that the tip probability increases by around 10-15% when the order was placed in this period. Therefore, the feature also has a moderate impact on the tip probability.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its moderate impact on tip probability.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4c64f166b3f09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hod_high_tip_probability.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec5589359cee5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Order Number Squared (order_number_squared)**\n",
    "**Description:** The feature represents the squared order number of the user. The order number squared is calculated by squaring the order number of the user. For example, an order number of 5 results in an order number squared of 25. This feature was created to capture the quadratic relationship between the order number and the tip probability, which was observed in the analysis of the order number.\n",
    "\n",
    "**Analysis:** This feature can't be properly analyzed and interpreted on its own, because it was created to allow linear models to capture the quadratic relationship between the order number and the tip probability. This relationship can only be observed when the feature is combined with the original order number feature, which is why the correlation is not that meaningful.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to the observations made in the analysis of the order number.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c70bd52a2619f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "order_number_squared.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda78205360232d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Department, Aisle, Product Tip Probability (department_tip_rate, aisle_tip_rate, product_tip_rate) - Dynamic Features**\n",
    "**Description:** These three features represents the tip probability for a specific order in dependence of the departments, aisles and products that are part of the order. The feature computation, the specific department ( for feature aisle & product exactly the same way ) tip probabilities are calculated across all user with all the data that is available in the fold (dynamic feature). In order to assign a value to the order, the departments of the respective order are selected and the mean value of their tip probability is calculated.\n",
    "\n",
    "**Analysis:** The three features have all an influence based on correlation and the order tip probability, but graph progression looks very similar. This is because they somehow represent the same context. There is a hierarchy between Departments, Aisles and Products. Within this context product is the most specific attribute whereas department is just more generalized.\n",
    "\n",
    "&rarr; **Because of the similarity between these three features, just product_tip_rate will be part of the model's feature set due to the highest correlation and the degree of specialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d86ef1cd92a1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "department_tip_rate.analyze_feature(orders_tip_features)\n",
    "aisle_tip_rate.analyze_feature(orders_tip_features)\n",
    "product_tip_rate.analyze_feature(orders_tip_features)  # TODO: Anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3708a55698b4eda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Order Frequency (order_frequency)**\n",
    "**Description:** The feature represents the frequency with which a user places his/her orders. For the current order, the order frequency is calculated by the mean of the days since the prior order for all previous orders and the current order. For example, if the current order is the third order of the user, the second order was placed 15 days ago and the first order was placed 17 days before the second order. Then, the order frequency for the user's current order is (15 + 17) / 2 = 16 days. This feature was created under the assumption that the frequency of orders of a user influences his/her tipping behavior (e.g. regular users tip more often).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around -0.1928 with the tip variable, which is moderate/low. The line graph on the right shows that the tip probability first increases and then decreases with a higher order frequency. The plot in the middle shows that the decrease in the tip probability is encompassed by a decrease in the order density. This reduces the significance of the decreasing tip probabilities a little, but as the density is nowhere near zero, the relationship is still valid.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its slight impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5842d7e2eb94b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "order_frequency.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7910e2e1567134",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Mean Ordered Rate (mean_ordered_rate)**\n",
    "**Description:** The feature represents the similarity of an order of a user compared to his/her previous orders. For the current order, the relative frequency each contained product was previously ordered is computed. Then, the mean is taken across these relative frequencies. For example, if the current order is the third order of the user and it contains bananas and milk with relative frequencies of 2/3 and 1/3, respectively. Then, the mean ordered rate for the user's current order is (2/3 + 1/3) / 2 = 1/2. This feature was created under the assumption that the similarity of products in an order compared to previous orders influences the tipping behavior of a user (e.g. similar orders get tipped with a higher probability).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around 0.0582 with the tip variable, which is not very strong. The line graph on the right shows that the tip probability first increases and then fluctuates with a higher mean ordered rate. The plot in the middle shows that the fluctuation in tip probability is encompassed by a decrease in the order density. This distorts the relationship of an increasing mean ordered rate with the tip probability.\n",
    "\n",
    "&rarr; **The feature will not be included in the model's feature set due to its low impact on tip probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f987fe5de199c45",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mean_ordered_rate.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b334896-3808-4bfe-82cb-5bfb1cb893ac",
   "metadata": {},
   "source": [
    "##### **Number of Days Since Tip (days_since_tip)**\n",
    "**Description:** The feature represents the number of days since a user gave a tip for an order. For the current order, the days since a prior order that was tipped are summed up. For example, if the current order is the fourth order of the user, the days since the third order are 8, the days since the second order are 4, and the second order was tipped. Then, the days since tip are 8 + 4 = 12 for the current order. This feature was created under the assumption that the number of days a user gave his/her last tip influences the tip probability (e.g. the higher the number of days the user has not tipped, the lower the probability the he/she tips for the current order).\n",
    "\n",
    "**Analysis:** The feature has a correlation of around -0.2971 with the tip variable, which seems to be significant for the tip probability. The line graph on the right shows a decreasing tip probability with an increasing number of days since the user gave his/her last tip.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its significant impact on tip probability. However, only models that can handle NaNs can handle this feature, since it will be NaN if a user has never tipped before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e907dc-1a8a-4eb1-a5b6-968dc868ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_since_tip.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93224df1-d37a-4c12-8d14-0153b7cd00d8",
   "metadata": {},
   "source": [
    "##### **Relative Number of Days Since Tip (rel_days_since_tip)**\n",
    "**Description:** The feature represents the number of days a tip is underdue/overdue for an order. For the current order, all previous orders are considered and the mean number of days between two orders that were tipped is computed. Then, for the current order, the number of days since a prior order that was tipped are summed up. As a result for the current order, this sum is deducted from the computed mean. Hence, if this difference is negative a tip is overdue and if this difference is positive a tip is underdue. For example, if the current order is the fourth order of the user, the first order was tipped, the second order was tipped after the next 5 days, the third order was tipped after the next 7 days, and the fourth order was placed after the next 7 days. Then, the sum of the number of days since a prior order was tipped is 7 days, since the third order was tipped and the fourth order was placed after the next 7 days. Moreover, the mean number of days between two orders that were tipped is (5 + 7) / 2 = 6 days. Then, a tip is overdue 1 day for the current (fourth) order, since 6 - 7 = -1. \n",
    "\n",
    "**Analysis:** The feature has a correlation of around -0.1205 with the tip variable, which is moderate/low. The line graph on the right together with the plot in the middle show that for the majority of orders, if a tip is due the tip probability is relatively high, while it decreases the more a tip becomes underdue and overdue.\n",
    "\n",
    "&rarr; **The feature will be included in the model's feature set due to its slight impact on tip probability. However, only models that can handle NaNs can handle this feature, since it will be NaN if a user has never tipped twice before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd47fe-c892-4268-952d-50cb017b30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_days_since_tip.analyze_feature(orders_tip_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8a34151c00934",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Comparison of features\n",
    "\n",
    "The comparison of all implemented features is based on correlations and regression coefficients. In the plots below describe the importance of features based on their correlations on the tip variable and the linear regression coefficients. All the features that were crafted have a different impact on the tip variable but with the knowledge of the overall analysis and the feature evaluation, some features can be categorized with the same label. \n",
    "\n",
    "- History of tip: tip_history, last_tip_sequence, \n",
    "- Alcohol products:\n",
    "- Order Time: \n",
    "- Product tip rates:  \n",
    "\n",
    "Based on the analysis of the features in the previous chapter, the following features were selected to be part of the model feature set. The decision is based on the analysis of the various plots, the correlation and the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d63d280802960",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_manager.calculate_feature_correlations()\n",
    "data_manager.analyse_linear_regression_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf0f49912ed6b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Training\n",
    "The model training consists of several steps and components:\n",
    "- **Cross-Validation:** A custom user-based time-series cross-validation strategy is used to split the data into training and test sets.\n",
    "- - **Feature Selection:** Based on the feature analysis, the most significant features, which also fit the model type, are selected.\n",
    "- **Training Pipeline:** For the hyperparameter tuning and the model training a pipeline is used, which is executed each fold of the cross-validation.\n",
    "    - **Dataset Selection:** A custom dataset selector is used which selects a pre-calculated dataset for the respective fold (avoids re-calculating dynamic features each fold). \n",
    "    - **Feature Scaling:** Depending on the model type, the features are scaled by a standard scaler.\n",
    "    - **Classifier:** The last step of the pipeline is the classifier, which is trained on the training data and used to predict the tip of the test data of the respective fold.\n",
    "- **Hyperparameter Tuning:** For hyperparameter tuning, a grid search is used, which tests different hyperparameter combinations and selects the best one based on a scoring metric. The grid search executes the training pipeline for each hyperparameter combination and each fold of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393b4d6e4906e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from data_management.cross_validation import LastOrderUserTSCVSplitter\n",
    "from data_management import DatasetSelector\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c63668093a5136",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Since many features, such as \"tip_history,\" depend on a user's order history, they frequently contain NaN values for a user's initial order. Consequently, the first order from each user is excluded from the dataset, since they provide little value to the model and would require additional handling to impute the missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41890d354a74adc1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_manager.remove_first_orders()\n",
    "orders_tip_test = data_manager.get_orders_tip_test().copy()\n",
    "orders_tip_train = data_manager.get_orders_tip_train().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdde8226661ef1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Cross-Validation\n",
    "To avoid data leakage, 2 custom user-based time-series cross-validation strategies were implemented:\n",
    "- **LastOrderUserTSCVSplitter:** This splitter splits the data based on the last order of each user. The most recent order of each user is used as test data, while all previous orders are used as training data. The test data of each fold is removed in all subsequent folds.\n",
    "- **UserTSCVSplitter:** This splitter uses a defined ratio of the most recent orders of each user as test data and the remaining orders as training data. For example, if the ratio is set to 0.2 and a user has 10 orders, the last 2 orders are used as test data and the first 8 orders are used as training data. The test data of each fold is removed in all subsequent folds. If it does not work out exactly (e.g. a user with 9 orders), then orders are selected probabilistically. For example, for a user with 9 orders and a ratio of 0.2, 1.8 orders should be selected. In this case the last order is selected, and the second last order is selected with a probability of 0.8. Also, the current size of the respective fold is used as reference for the selection, which means that the ratio of test and training data remains constant (e.g. 80:20).\n",
    "\n",
    "Then number of folds for the cross-validation was set to 5,to limit the computational effort and to ensure that the model is trained on a sufficient amount of data.\n",
    "After some tests, it was decided to use **LastOrderUserTSCVSplitter** for the model training, because it is more similar to the final prediction task, as only the tip for last orders must be predicted.\n",
    "\n",
    "The splitter splits the training data into 5 folds and re-calculates the dynamic features for each fold, to avoid data leakage. To incorporate the information of the first orders, without using them for training, the first orders are added for the calculation of the dynamic features and are removed again afterward. The prepared splits are then exported to the local file system, to avoid re-calculating them each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc3ca1939cc3ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the second line to re-calculate the splits if necessary (this will take a long time)\n",
    "\n",
    "last_order_user_tscv_splitter = LastOrderUserTSCVSplitter(data_manager, n_splits=5)\n",
    "# last_order_user_tscv_splitter.export_splits('data/prepared_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09d534bc54ea9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next lines to use the UserTSCVSplitter instead (this will take a long time)\n",
    "\n",
    "# user_tscv_splitter = UserTSCVSplitter(data_manager, n_splits=5, validation_set_ratio=0.2, seed=42)\n",
    "# user_tscv_splitter.export_splits('data/prepared_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543ae566abb243f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Feature Selection TODO: List features\n",
    "Based on the feature analysis different feature sets were created:\n",
    "- **Core Features:** Contains only the most significant features, which were selected based on the feature analysis.\n",
    "- **Ordinal Features:** Contains the ordinal features\n",
    "- **Ordinal Features Replacement:** Contains the binary replacements for the ordinal features.\n",
    "- **Features with NaN Values:** Contains all features, which contain many NaN values, because a tip is required to be present in the previous order history to calculate the feature.\n",
    "\n",
    "The following feature sets were created to be used for the different classes of classifiers:\n",
    "- **Numerical-only Classifiers:** Contains the core features and the ordinal features replacements.\n",
    "- **Numerical & Categorical Classifiers:** Contains the core features, the ordinal features and the features with NaN values, as decision trees can handle NaN values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d23a7c69b56f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_core = ['order_number', 'days_since_prior_order', 'tip_history', 'customer_lifetime', 'reordered_ratio',\n",
    "                 'order_frequency', 'last_tip_sequence', 'contains_alcohol', 'prev_order_tipped',\n",
    "                 'prev_tipped_products_ratio', 'sim_orders_tip_ratio', 'aisle_tip_rate', 'dept_tip_rate',\n",
    "                 'product_tip_rate', 'order_number_squared']\n",
    "\n",
    "features_ordinal = ['order_dow', 'order_hour_of_day']\n",
    "features_ordinal_replacement = ['dow_high_tip_probability', 'hod_high_tip_probability']\n",
    "features_with_na = ['rel_days_since_tip', 'days_since_tip']\n",
    "\n",
    "features_numeric = features_core + features_ordinal_replacement\n",
    "features = features_core + features_ordinal + features_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2bf8861dbbd58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "###  Classifiers (Numerical-only)\n",
    "\n",
    "The following classifiers, which only support numerical features, were selected for the model training:\n",
    "- **Logistic Regression**\n",
    "- **Multi-Layer Perceptron**\n",
    "- **Gaussian Naive Bayes**\n",
    "\n",
    "The data is prepared using only the feature set for the numerical-only classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e2cceea354d29",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The custom splitter imports the prepared splits from the local file system and provides the splits as dictionary, where the indices of all orders of the respective fold are hashed and used as keys. In the pipeline, the dataset selector then exchanges the dataset of the current fold with the respective prepared fold by hashing the indices of dataset in the current fold and retrieving the corresponding prepared fold from the dictionary. This way the correct dataset is selected for each fold. This dataset selector is needed as a workaround to dynamically exchange the dataset, because sklearn only allows custom splitters to return indices and not the actual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65959eb36e7586f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prepared_splits_numeric = last_order_user_tscv_splitter.import_splits('data/prepared_data/', features_numeric)\n",
    "X_num = orders_tip_train[features_numeric]\n",
    "y_num = orders_tip_train['tip'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5944c0d3d00052b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "The logistic regression model is trained using the numerical-only feature set. The hyperparameters are tuned using a grid search with 5-fold cross-validation. The hyperparameters that are tuned are the regularization parameter C and the solver as well as the maximum iterations. The best hyperparameters are selected based on the accuracy score. The best hyperparameters reached a mean accuracy of 0.802477 and a top accuracy of 0.813330.\n",
    "\n",
    "The logistic regression is based on the regression of a linear function, which is then transformed into a probability using the logistic function. The model is used to predict the probability of a tip based on the input features. It was interesting to observe the coefficients of the logistic regression, as they provide information about the importance of the features for the prediction. The model is simple and easy to use but lacks a little in comparison to more complex models like neural networks and XGboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8953aefa2b1bf38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from data_management import eval_logreg\n",
    "from data_management import get_best_cv_scores\n",
    "\n",
    "pipeline_steps = [('selector', DatasetSelector(prepared_splits_numeric)),\n",
    "                  ('scaler', StandardScaler()),\n",
    "                  ('logreg', LogisticRegression())]\n",
    "\n",
    "pipeline = Pipeline(pipeline_steps, verbose=False)\n",
    "\n",
    "# param_grid = {\n",
    "#     'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#     'logreg__solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "# } -> Mean: 0.802477 Top: 0.813330\n",
    "param_grid = {\n",
    "    'logreg__max_iter': [1000],\n",
    "    'logreg__C': [0.001],\n",
    "    'logreg__solver': ['sag'],\n",
    "}\n",
    "\n",
    "grid_search_clf_log = GridSearchCV(pipeline, param_grid, cv=last_order_user_tscv_splitter, scoring='accuracy',\n",
    "                                   verbose=1,\n",
    "                                   n_jobs=5, return_train_score=True)\n",
    "grid_search_clf_log.fit(X_num, y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7f7819bfbdb96",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eval_logreg(grid_search_clf_log, features_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475b9dcb7616dfb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores_log = get_best_cv_scores(grid_search_clf_log)\n",
    "scores_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e6dd89de5cd16",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Multi-Layer Perceptron\n",
    "\n",
    "The multi-layer perceptron model is trained using the numerical-only feature set. The hyperparameters are tuned using a grid search with 5-fold cross-validation. The hyperparameters that are tuned are the hidden layer sizes, the activation function, the regularization parameter alpha and the maximum number of iterations. The best hyperparameters are selected based on the accuracy score. The best hyperparameters reached a mean accuracy of 0.803884 and a top accuracy of 0.814843.\n",
    "\n",
    "The multi-layer perceptron is a feedforward neural network, which consists of multiple layers of nodes. The model is able to capture non-linear relationships between the features, which can be an advantage compared to linear models like logistic regression. For the hyperparameter tuning we choose to keep it simple with few layers and nodes, as the dataset is not very large and complex. The model had a good performance and was able to capture the relationships between the features well. It is a good choice for the prediction of the tip probability and is a contender for the best model for the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e66c29da12c48c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipeline_steps = [('selector', DatasetSelector(prepared_splits_numeric)),\n",
    "                  ('scaler', StandardScaler()),\n",
    "                  ('mlp', MLPClassifier())]\n",
    "\n",
    "pipeline = Pipeline(pipeline_steps, verbose=False)\n",
    "\n",
    "# param_grid = {\n",
    "#     'mlp__hidden_layer_sizes': [(10,), (50,), (100,), (5, 5), (10, 10), (25, 25)],\n",
    "#     'mlp__activation': ['relu'],\n",
    "#     'mlp__alpha': [0.001, 0.01, 0.1],\n",
    "#     'mlp__max_iter': [100, 500, 750]\n",
    "# } -> Top: 0.814843, Mean: 0.803884\n",
    "\n",
    "param_grid = {'mlp__activation': ['relu'],\n",
    "              'mlp__alpha': [0.01],\n",
    "              'mlp__hidden_layer_sizes': [(25, 25)],\n",
    "              'mlp__max_iter': [750]}\n",
    "\n",
    "grid_search_clf_mlp = GridSearchCV(pipeline, param_grid, cv=last_order_user_tscv_splitter, scoring='accuracy',\n",
    "                                   verbose=1,\n",
    "                                   n_jobs=5, return_train_score=True)\n",
    "grid_search_clf_mlp.fit(X_num, y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3c86185f0aca8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores_mlp = get_best_cv_scores(grid_search_clf_mlp)\n",
    "scores_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f308f8d3dbc957",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "The Gaussian Naive Bayes model is trained using the numerical-only feature set. The hyperparameters are tuned using a grid search with 5-fold cross-validation. The hyperparameters that are tuned is the variation smoothing parameter. The best hyperparameters are selected based on the accuracy score. The best hyperparameters reached a mean accuracy of 0.784138.\n",
    "\n",
    "The Gaussian Naive Bayes model is based on Bayes' theorem and assumes that the features are independent. The first time the model was trained, it had the worst performance of all models so far. This lead to excluding the model from further analysis, as it seems that the model is not able to capture the relationships between the features well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e3a9238552b9b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipeline_steps = [('selector', DatasetSelector(prepared_splits_numeric)),\n",
    "                  ('scaler', StandardScaler()),\n",
    "                  ('nb', GaussianNB())]\n",
    "\n",
    "pipeline = Pipeline(pipeline_steps, verbose=False)\n",
    "# param_grid = {\n",
    "#     'nb__var_smoothing': [1e-9, 1e-10, 1e-11]\n",
    "# } -> Mean: 0.784138\n",
    "param_grid = {\n",
    "    'nb__var_smoothing': [1e-9]  # Variation smoothing parameter\n",
    "}\n",
    "\n",
    "grid_search_clf_bayes = GridSearchCV(pipeline, param_grid, cv=last_order_user_tscv_splitter, scoring='accuracy',\n",
    "                                     verbose=1,\n",
    "                                     n_jobs=5, return_train_score=True)\n",
    "grid_search_clf_bayes.fit(X_num, y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57683ae0b7c39376",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores_bayes = get_best_cv_scores(grid_search_clf_bayes)\n",
    "scores_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389e08e407a2285",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Classifiers (Numerical & Categorical)\n",
    "The following classifiers, which support numerical and categorical features, were selected for the model training:\n",
    "- **Decision Tree**\n",
    "- **XGBoost**\n",
    "\n",
    "The data is prepared using only the feature set for the classifiers, that can handle numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a4db227221af0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prepared_splits = last_order_user_tscv_splitter.import_splits('data/prepared_data/', features)\n",
    "X = orders_tip_train[features]\n",
    "y = orders_tip_train['tip'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756f6632343767c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Decision Tree\n",
    "\n",
    "The decision tree model is trained using the feature set, which contains numerical and categorical features. Furthermore, the features were not scaled because it is not necessary for a Decision Tree. The hyperparameters are tuned using a grid search with 5-fold cross-validation. The hyperparameters that are tuned are the criterion, the maximum depth and the minimum samples leaf. The best hyperparameters are selected based on the accuracy score. The best hyperparameters reached a mean accuracy of 0.802.\n",
    "\n",
    "The decision tree model is a tree-like model, which splits the data based on the features to predict the target variable. The model is able to capture non-linear relationships between the features and is straightforward to interpret. Compared to the XGBoost model, the decision tree model is less complex and has a lower performance, which is why we decided to prioritize the XGBoost model for the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a01079e279e7bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_steps = [('selector', DatasetSelector(prepared_splits)),\n",
    "                  ('decision_tree', DecisionTreeClassifier())]\n",
    "\n",
    "pipeline = Pipeline(pipeline_steps, verbose=False)\n",
    "\n",
    "# param_grid = {\n",
    "#     'decision_tree__criterion': ['entropy'],\n",
    "#     'decision_tree__max_depth': [2 ** i for i in range(5, 15)],\n",
    "#     'decision_tree__min_samples_leaf': [2 ** i for i in range(5, 20)]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'decision_tree__criterion': ['entropy'],\n",
    "    'decision_tree__max_depth': [10],\n",
    "    'decision_tree__min_samples_leaf': [500]\n",
    "}\n",
    "\n",
    "grid_search_clf_dtree = GridSearchCV(pipeline, param_grid, cv=last_order_user_tscv_splitter, scoring='accuracy',\n",
    "                                     verbose=1,\n",
    "                                     n_jobs=5, return_train_score=True)\n",
    "grid_search_clf_dtree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358f6be251f148",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores_dtree = get_best_cv_scores(grid_search_clf_dtree)\n",
    "scores_dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f022f1adc52a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "The XGBoost model is trained using the feature set, which contains numerical and categorical features. Furthermore, the features were not scaled because it is not necessary for a XGBoost Decision Tree. The hyperparameters are tuned using a grid search with 5-fold cross-validation. The hyperparameters that are tuned are the number of estimators, the maximum depth, the learning rate and the subsample. The best hyperparameters are selected based on the accuracy score. The best hyperparameters reached a mean accuracy of 0.805.\n",
    "\n",
    "The XGBoost model is an ensemble learning method, which is based on decision trees. The model is able to capture non-linear relationships between the features and is known for its high performance. The model is more complex than the decision tree model and is able to capture the relationships between the features better. The XGBoost model is a good choice for the prediction of the tip probability and is a contender for the best model for the prediction task. Furthermore, it reached the highest mean accuracy of all models so far and is preferred for the final prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda5f1832468159",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipeline_steps = [('selector', DatasetSelector(prepared_splits)),\n",
    "                  ('xgb', XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "pipeline = Pipeline(pipeline_steps, verbose=False)\n",
    "\n",
    "# param_grid = {'xgb__n_estimators': [2 ** i for i in range(2, 8)],\n",
    "#               'xgb__max_depth': [2 ** i for i in range(2, 8)],\n",
    "#               'xgb__learning_rate': [0.01, 0.1],\n",
    "#               'xgb__subsample': [0.4, 0.6, 0.8]}\n",
    "#               -> Mean: 0.805 \n",
    "\n",
    "param_grid = {\n",
    "    'xgb__learning_rate': [0.1],\n",
    "    'xgb__max_depth': [5],\n",
    "    'xgb__n_estimators': [100],\n",
    "    'xgb__subsample': [0.6]\n",
    "}\n",
    "\n",
    "grid_search_clf_xgboost = GridSearchCV(pipeline, param_grid, cv=last_order_user_tscv_splitter, scoring='accuracy',\n",
    "                                       verbose=1,\n",
    "                                       n_jobs=1, return_train_score=True)\n",
    "grid_search_clf_xgboost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc789ed3bdb1ef4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "scores_xgboost = get_best_cv_scores(grid_search_clf_xgboost)\n",
    "scores_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e49680828e3d5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Accuracy Prediction\n",
    "The prediction of the accuracy, which is expected to be achieved by the best model for the test set, is done using the accuracy scores, which have been calculated during the cross-validation. \n",
    "\n",
    "By looking at the scores of the different folds, one recognizes that each fold the accuracy test score decreases. This can be explained by the custom time-series cross-validation strategy. Because the most recent order of each user is used as test data only once and is then removed in all subsequent folds, the model is trained on less data each fold, which can lead to a decrease in accuracy. Additionally, the order history of each user becomes shorter with each fold. Because many features are computed based on the order history of the respective user, the features of the test data become less informative with each fold, which is why the model's predictions, based on these features, become less accurate. Following this logic, the accuracy of the model for the test set should be higher than the accuracy scores of the first fold. The reason for this is that the model is trained on the entire dataset and the features of the test data are more informative, since the order history of each user contains 1 order more than in the first fold.\n",
    "\n",
    "Therefore, a linear regression model is fitted to the accuracy scores of the different folds to model the relationship between the length of the order history and the accuracy score. The regression model is then used to predict the accuracy of the classifier for the test set. The visualization shows that the model's accuracy increases linearly with the length of the order history. Therefore, it seems reasonable to predict the accuracy of the model for the test set by following this trend using the linear regression model.\n",
    "\n",
    "In the plot below, the accuracy scores of the train (blue) and test (orange) data are shown for each fold. The estimated accuracy of the model for the final test set is depicted in green.\n",
    "\n",
    "**Note**: It can be observed, that the accuracy for the test set is higher than the accuracy of the train set, which is unusual. A possible explanation for this could be that it's easier for the model to predict the tip for the test set, because there is a longer order history, based on which the features are calculated. The train set on the other hand contains more orders, with a shorter history (e.g. order 2,3,4), which makes the features of these orders less informative and the prediction more difficult.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86158662aa078aca",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from data_management import estimate_accuracy\n",
    "\n",
    "accuracy_log = estimate_accuracy(grid_search_clf_log)\n",
    "accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19428f4d26239ae8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "accuracy_mlp = estimate_accuracy(grid_search_clf_mlp)\n",
    "accuracy_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494274ec1440497",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "accuracy_bayes = estimate_accuracy(grid_search_clf_bayes)\n",
    "accuracy_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffa3025342203e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "accuracy_dtree = estimate_accuracy(grid_search_clf_dtree)\n",
    "accuracy_dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e3a6bc0295285",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "accuracy_xgboost = estimate_accuracy(grid_search_clf_xgboost)\n",
    "accuracy_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0560d8f57599",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prediction\n",
    "\n",
    "The best model is used to predict the tip for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998f0df62cfaf24",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Select best model\n",
    "best_estimator = grid_search_clf_xgboost.best_estimator_\n",
    "X_test = orders_tip_test[features]\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "orders_tip_test['tip'] = y_pred\n",
    "orders_tip_test['tip'].value_counts()\n",
    "\n",
    "orders_tip_test_csv = pd.merge(tip_test.drop('tip', axis=1), orders_tip_test[['order_id', 'tip']], on='order_id',\n",
    "                               how='left')\n",
    "orders_tip_test_csv.rename(columns={tip_test.columns[0]: ''}, inplace=True)\n",
    "orders_tip_test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3db4fe8464564",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The predicted tip for the test set is saved as a CSV file and exported to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ef2ac33233188",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "orders_tip_test_csv.to_csv(os.path.join(DATA_DIR, 'submission.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
